\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}

\title{
  \vspace{-2cm}
  CS 224n Assignment \#4 \\
  \author{Yoshihiro Kumazawa}
}

\begin{document}
\maketitle
\begin{enumerate}[label=\textbf{\arabic*.}]
  \item \textbf{Neural Machine Translation with RNNs}
  \begin{enumerate}[label=(\alph*)]
    \item See \texttt{utils.py}.
    \item See \texttt{model\_embeddings.py}.
    \item See \texttt{nmt\_model.py}.
    \item See \texttt{nmt\_model.py}.
    \item See \texttt{nmt\_model.py}.
    \item See \texttt{nmt\_model.py}.
    \item The masked logits are made $-\infty$ and hence do not affect the softmax calculation of the other logits. Those masks are put on the hidden states from the padded words, which no attention should be paid to.
    \item (Missing)
    \item The modelâ€™s corpus BLEU Score was 35.83.
    \item One advantage of dot product attention compared to multiplicative attention is that it is less prone to overfitting since it does not have weight parameters. One disadvantage is its lower expressivity. One advantage of additive attention compared to multiplicative attention is that it can learn not to be affected by the hidden state very much by letting $W_1$ small. One disadvantage is that it is more prone to overfitting.
  \end{enumerate}
  \item \textbf{Analyzing NMT Systems}
  \begin{enumerate}[label=(\alph*)]
    \item
    \begin{enumerate}[label=\roman*.]
      \item
      \item
      \item
      \item
      \item
      \item
    \end{enumerate}
    \item
    \item
    \begin{enumerate}[label=\roman*.]
      \item
      \item
      \item
      \item
    \end{enumerate}
  \end{enumerate}
\end{enumerate}
\end{document}
