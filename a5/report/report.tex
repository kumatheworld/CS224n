\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}

\title{
  \vspace{-2cm}
  CS 224n Assignment \#5 \\
  \author{Yoshihiro Kumazawa}
}

\begin{document}
\maketitle
\begin{enumerate}[label=\textbf{\arabic*.}]
  \item \textbf{Character-based convolutional encoder for NMT}
  \begin{enumerate}[label=(\alph*)]
    \item Convolutional architectures can operate over variable length input too since convolutional layers slide fixed-sized windows over input unlike linear layers.
    \item The size of the padding should be 1 so that the padded vector will have size at least 5. Indeed, $m_\text{word}$ could be 1 if all words in a batch happen to be some characters of length 1 like 'a', in which case we have $\mathbf{x}'_\text{padded}\in\mathbb{Z}^3$.
    \item The highway layer makes it possible to combine local features and global features. In other words, it matches our intuition that we can sometimes understand the meaning of a word by just looking at a little chunk of consecutive characters at a time but it is sometimes better to consider the whole characters in it at once. In order to simplify the network semantics in the beginning of training, I would initialize $\bm{b}_\text{gate}$ to be negative.
    \item Transformers are easier to parallelize and faster to train.
    \item See \texttt{vocab.py}.
    \item For the highway network implementation, see \texttt{highway.py}. I added a function \texttt{question\_1f\_sanity\_check()} in \texttt{sanity\_check.py} to test the following expected properties.
    \begin{itemize}
      \item The output size is correct for a given input.
      \item $\bm{x}_\text{highway}=\bm{x}_\text{conv\_out}$ when $\bm{x}_\text{gate}=0$, which is checked by making $\bm{b}_\text{gate}=-\infty$.
      \item $\bm{x}_\text{highway}=\bm{x}_\text{proj}$ when $\bm{x}_\text{gate}=1$, which is checked by making $\bm{b}_\text{gate}=\infty$.
      \item $\bm{x}_\text{highway}=\bm{x}_\text{conv\_out}$ when the projection layer is the identity function.
    \end{itemize}
    In addition, I checked if $\bm{x}_\text{gate}$ is initialized to be negative by computing the mean 4 times. (see my answer for 1 (c) above).
    \item For the convolutional network implementation, see \texttt{cnn.py}. I added a function \texttt{question\_1g\_sanity\_check()} in \texttt{sanity\_check.py} to test the following expected properties.
    \begin{itemize}
      \item The sizes of input channels, output channels, kernels and padding of the convolutional layer are correct.
      \item The output size is correct for a given input.
    \end{itemize}
    \item See \texttt{model\_embeddings.py}. I do not provide any additional test for it.
    \item See \texttt{nmt\_model.py}.
    \item See \texttt{outputs/test\_outputs\_local\_q1.txt}. The BLEU score is 99.67.
  \end{enumerate}
  \item \textbf{Character-based LSTM decoder for NMT}
  \begin{enumerate}[label=(\alph*)]
    \item See \texttt{char\_decoder.py}.
    \item See \texttt{char\_decoder.py}.
    \item See \texttt{char\_decoder.py}.
    \item See \texttt{outputs/test\_outputs\_local\_q2.txt}. The BLEU score is 40.92.
    \item See \texttt{outputs/test\_outputs.txt}. The BLEU score is 34.99. So would I get 0 point here? So sad...
  \end{enumerate}
  \item \textbf{Analyzing NMT Systems}
  \begin{enumerate}[label=(\alph*)]
    \item Traducir, traduzco and traduce are in \texttt{vocab.json}, while traduces, traduzca and traduzcas are not. This is bad for word-based NMT models from Spanish to English because they will not directly translate such words, which contain not only the essential meanings but also information about subjects or tense. Our character-based model will possibly overcome the problem since it is based on hidden states of the LSTM, which expectedly have some information about the right words to produce. Moreover, as in the above example of Spanish words some of which are in the vocabulary and others are not, words that mean similar things tend to be made of similar characters, where our model would conceivably work well.
    \item
    \begin{enumerate}[label=\roman*.]
      \item Below are the (word $\rightarrow$ closest neighbor) pairs.
        \begin{itemize}
          \item financial $\rightarrow$ economic
          \item neuron $\rightarrow$ nerve
          \item Francisco $\rightarrow$ san
          \item naturally $\rightarrow$ occurring
          \item expectation $\rightarrow$ norms
        \end{itemize}
      \item Below are the (word $\rightarrow$ closest neighbor) pairs.
        \begin{itemize}
          \item financial $\rightarrow$ vertical
          \item neuron $\rightarrow$ Newton
          \item Francisco $\rightarrow$ France
          \item naturally $\rightarrow$ practically
          \item expectation $\rightarrow$ exception
        \end{itemize}
      \item The CharCNN models similarity in spelling whereas Word2Vec models semantic similarity. This is due to how those models are trained. With the CharCNN, similarly spelled words will have similar representations after convolutions, whereas semantically similar words will be embedded closely with Word2Vec since it tries to model words by nearby words they are placed with in sentences.
    \end{enumerate}
    \item Acceptable case (line 7,657):
    \begin{enumerate}[label=\arabic*.]
      \item Entonces, funcion", dije.
      \item "So, then it worked," I said.
      \item So, <unk> I said.
      \item So, it worked ,"I said.
      \item Since words with quotation symbols do not show up in the vocabulary, the CharCNN model tried to find words rather than producing \texttt{<UNK>}.
    \end{enumerate}
    Incorrect case (line 8):
    \begin{enumerate}[label=\arabic*.]
      \item Un amigo mo hizo eso -- Richard Bollingbroke.
      \item A friend of mine did that -- Richard Bollingbroke.
      \item A friend of mine did that -- Richard <unk>
      \item A friend of mine did that -- Richard Bollywood.
      \item It tried to translate the word Bollingbroke, which is not in the vocaburary, but ended up producing Bollingbroke, which is similar in spelling but means a totally different thing.
    \end{enumerate}
  \end{enumerate}
\end{enumerate}
\end{document}
