\documentclass[12pt, dvipdfmx]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{amsfonts}

\title{
  \vspace{-2cm}
  CS 224n Assignment \#3 \\
  \author{Yoshihiro Kumazawa}
}

\begin{document}
\maketitle
\begin{enumerate}[label=\textbf{\arabic*.}]
\item \textbf{Machine Learning \& Neural Networks}
\begin{enumerate}[label=(\alph*)]
\item
\begin{enumerate}[label=\roman*.]
\item When $\beta_1$ is large, $\bm{m}$ relies more on the history of the past gradients rather than the new one. For example, if $\beta_1=0.9$, the contribution of the new gradient to weight update is only 10\% of that without momentum.
\item The model parameters with smaller gradients will get larger updates. When the variance of the gradients is high, Adam might help the parameters go in the direction of small gradients, which the vanilla update strategy would ignore due to rounding errors.
\end{enumerate}
\item
\begin{enumerate}[label=\roman*.]
\item $\gamma$ must equal $1/(1-p_\mathrm{drop})$ to make $\mathbb{E}_{p_\mathrm{drop}}[\bm{h}_\mathrm{drop}]_i=h_i$.
\item
\end{enumerate}
\end{enumerate}
\item \textbf{Neural Transition-Based Dependency Parsing}
\begin{enumerate}[label=(\alph*)]
\item
\item
\item
\item
\item
\item
\end{enumerate}
\end{enumerate}
\end{document}
